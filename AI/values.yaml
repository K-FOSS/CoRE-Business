

env: prod

hub: true

cluster:
  name: k8s
  domain: cluster.local

datacenter: dc1

region: yxl

global:
  test: 'test'


mcpbox:
  enabled: true

  image: quay.io/mudler/localagi-mcpbox

  tag: main

localagi:
  enabled: true

  persistence:
    enabled: true

  image: quay.io/mudler/localagi
  tag: main


wyoming:
  enabled: true

  replicas: 2

  service:
    enabled: true

  image: ghcr.io/roryeckel/wyoming_openai
  tag: latest

  tts:
    endpoint: https://tts.int.mylogin.space/v1

    models:
      - tts-1-hd

    streamingModels:
      - tts-1-hd

    voices: 'af_bella'

  stt:
    endpoint: https://stt.int.mylogin.space/v1

    models:
      - Systran/faster-distil-whisper-small.en

    streamingModels: []

  logs:
    level: DEBUG

  languages: 'en'

speeches:
  enabled: true

  image: ghcr.io/speaches-ai/speaches
  tag: 0.8.0-rc.3-cuda-12.4.1

  runtimeClassName: nvidia

  externalBackend:
    - name: hpc3-windows
      endpoint:
        ip:
          address: 172.31.192.12
          port: 8001

  nodeSelector:
    gpu-node: 'true'


tts:
  enabled: false

  image: ghcr.io/remsky/kokoro-fastapi-gpu
  tag: v0.2.4

  runtimeClassName: nvidia

  nodeSelector:
    gpu-node: 'true'

  externalBackend:
    - name: hpc3-windows
      endpoint:
        ip:
          address: 172.31.192.12
          port: 8880


externalSecrets:
  enabled: true

  secretStoreRef:
    kind: ClusterSecretStore
    name: mainvault-core

gpustack:
  enabled: true

  server:
    enabled: true

    image: gpustack/gpustack
    tag: latest-cuda12.8

    pod:
      runtimeClassName: nvidia
    
    service:
      enabled: true

    gateway:
      enabled: true
      httproute:
        enabled: true

        domains:
          - gpustack.mylogin.space

        labels:
          test: 'true'

    nodeSelector:
      kubernetes.io/hostname: hpc2

  workers:
    - name: cpu-worker
      enabled: true
      image: gpustack/gpustack
      tag: latest-cpu
      pullPolicy: Always

      args:
        - --server-url
        - https://gpustack.mylogin.space
        - --token
        - <path:CORE0_SITE1/data/AI/GPUStack#Token>

      controller:
        type: daemonset

      nodeSelector:
        gpu-node: 'false'

    - name: gpu-worker
      enabled: true
      image: gpustack/gpustack
      tag: latest-cuda12.8
      pullPolicy: Always

      statefulset:
        # -- Set podManagementPolicy, valid values are Parallel and OrderedReady (default).
        podManagementPolicy: Parallel

        # -- Used to create individual disks for each instance.
        volumeClaimTemplates:
          - name: backup
            labels: {}
            annotations: {}
            globalMounts:
              - path: /backup
            accessMode: "ReadWriteOnce"
            size: 2Gi
            storageClass: ssd-storage


      pod:
        runtimeClassName: nvidia

      args:
        - --server-url
        - https://gpustack.mylogin.space
        - --token
        - <path:CORE0_SITE1/data/AI/GPUStack#Token>

      controller:
        type: daemonset

      nodeSelector:
        gpu-node: 'true'

localai:
  enabled: false

  nodeSelector:
    kubernetes.io/hostname: hpc3

docker:
  mcp:
    enabled: true
  kubedock:
    enabled: false

    rbac:
      create: false

llama:
  #
  # TODO: Test Servers with GRPC backends
  #
  server:
    enabled: true

  models: []
    # - name: qwen3-large
    #   container:
    #     image:
    #       repository: kristianfoss/llama
    #       tag: 'test'
    #       pullPolicy: Always
    
    #   path: /models/Qwen_Qwen3-30B-A3B-Q4_0.gguf
    #   contextSize: 2048
    #   threads: 12
    #   batchSize: 256

    # - name: qwen3-r1
    #   container:
    #     image:
    #       repository: kristianfoss/llama
    #       tag: 'test'
    #       pullPolicy: Always
    
    #   path: /models/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-Q8_0.gguf
    #   contextSize: 8192
    #   threads: 12
    #   batchSize: 512

    # - name: nsfw-chat
    #   container:
    #     image:
    #       repository: kristianfoss/llama
    #       tag: 'test'
    #       pullPolicy: Always
    
    #   path: /models/L3-Nymeria-Maid-8B.Q4_K_M.gguf
    #   contextSize: 8192
    #   threads: 6
    #   batchSize: 512

    # - name: small-router
    #   container:
    #     image:
    #       repository: kristianfoss/llama
    #       tag: 'test'
    #       pullPolicy: Always
    
    #   path: /models/Qwen_Qwen3-4B-Thinking-2507-Q8_0.gguf
    #   contextSize: 8192
    #   threads: 6
    #   batchSize: 512

owui:
  enabled: true

  psql:
    enabled: true

    host: psql.core-home1-talos-prod.home1.yvr.mylogin.space

  mcpo:
    enabled: true

mcp:
  enabled: true

  hub:
    enabled: true

  containers:
    - name: brave-search
      image: mcp/brave-search
      tag: latest

      env:
        - name: BRAVE_API_KEY
          valueFrom:
            secretKeyRef:
              name: '{{ include "bjw-s.common.lib.chart.names.fullname" $ }}-core-business-ai-brave-search'
              key: APIKey

        - name: BRAVE_MCP_TRANSPORT
          value: 'http'

  frr:
    enabled: true

  docker:
    enabled: true

  search:
    enabled: true

ai:
  namespace:
    name: core-ai-testing

    create: true

pubgateway:
  name: main-gw
  namespace: core-prod
  sectionName: https-myloginspace



oauth:
  indexKey: openwebui
  scopes: ''
  groups: []
  authentik:
    urlBase: https://idp.mylogin.space


gateway:
  enabled: true

  tenant: testing

  service:
    labels:

    annotations:
      purelb.io/allow-shared-ip: ai.int.mylogin.space
      purelb.io/service-group: core-anycast
      purelb.io/allow-local: 'true'
      purelb.io/addresses: 10.1.1.19

  labels:
    wan-mode: 'public'
    lan-mode: 'private'
    resolvemy.host/gw: 'ai-gw'
    resolvemy.host/gw-ns: '{{ .Release.Namespace }}'
    resolvemy.host/security: 'private'
    resolvemy.host/env: 'prod'

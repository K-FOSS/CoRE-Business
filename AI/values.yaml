

env: prod

hub: true

ai:
  namespace:
    name: core-ai-testing

    create: true

cluster:
  name: k8s
  domain: cluster.local


gateway:
  name: main-gw
  namespace: core-prod
  sectionName: https-myloginspace

datacenter: dc1

region: yxl

# defaultPodOptions:
#   labels:
#     logs: loki-myloginspace

controllers:
  docker:
    # For Deployments, valid values are Recreate (default) and RollingUpdate.
    # For StatefulSets, valid values are OnDelete and RollingUpdate (default).
    # DaemonSets/CronJobs ignore this.
    strategy: RollingUpdate

    serviceAccount:
      identifier: kubedock

    containers:
      kubedock:
        image:
          repository: joyrex2001/kubedock
          tag: 'latest'
          pullPolicy: IfNotPresent

        # args:
        #   - --config=/config/config.json

        # env:
        #   - name: DOCKERD_ROOTLESS_ROOTLESSKIT_FLAGS
        #     value: '-p 0.0.0.0:2376:2376/tcp'

        ports:
          - name: http
            containerPort: 2475


  openwebuimcp:
    # For Deployments, valid values are Recreate (default) and RollingUpdate.
    # For StatefulSets, valid values are OnDelete and RollingUpdate (default).
    # DaemonSets/CronJobs ignore this.
    strategy: RollingUpdate

    name: openwebuimcp

    containers:
      docker:
        image:
          repository: docker
          tag: '28-dind-rootless'
          pullPolicy: IfNotPresent

        securityContext:
          privileged: true

        env:
          - name: DOCKERD_ROOTLESS_ROOTLESSKIT_NET
            value: 'host'

          - name: DOCKERD_ROOTLESS_ROOTLESSKIT_FLAGS
            value: ''

        # args:
        #   - 

        ports:
          - name: http
            containerPort: 2376

      mcpo:
        image:
          repository: kristianfoss/openwebui
          tag: 'core-mcp'
          pullPolicy: IfNotPresent

        command:
          - sh
          - -c
          - "until [ -S /var/run/docker.sock ]; do sleep 1; done; exec mcpo --config=/config/config.json"

        ports:
          - name: http
            containerPort: 8000

        # -- Probe configuration
        # -- [[ref]](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)
        # @default -- See below
        probes:
          # -- Liveness probe configuration
          # @default -- See below
          liveness:
            # -- Enable the liveness probe
            enabled: false
            # -- Set this to `true` if you wish to specify your own livenessProbe
            custom: false
            # -- sets the probe type when not using a custom probe
            # @default -- "TCP"
            type: TCP
            # -- The spec field contains the values for the default livenessProbe.
            # If you selected `custom: true`, this field holds the definition of the livenessProbe.
            # @default -- See below
            spec:
              initialDelaySeconds: 0
              periodSeconds: 10
              timeoutSeconds: 1
              failureThreshold: 3

          # -- Redainess probe configuration
          # @default -- See below
          readiness:
            # -- Enable the readiness probe
            enabled: false
            # -- Set this to `true` if you wish to specify your own readinessProbe
            custom: false
            # -- sets the probe type when not using a custom probe
            # @default -- "TCP"
            type: TCP
            # -- The spec field contains the values for the default readinessProbe.
            # If you selected `custom: true`, this field holds the definition of the readinessProbe.
            # @default -- See below
            spec:
              initialDelaySeconds: 0
              periodSeconds: 10
              timeoutSeconds: 1
              failureThreshold: 3

          # -- Startup probe configuration
          # @default -- See below
          startup:
            # -- Enable the startup probe
            enabled: false
            # -- Set this to `true` if you wish to specify your own startupProbe
            custom: false
            # -- sets the probe type when not using a custom probe
            # @default -- "TCP"
            type: TCP
            # -- The spec field contains the values for the default startupProbe.
            # If you selected `custom: true`, this field holds the definition of the startupProbe.
            # @default -- See below
            spec:
              initialDelaySeconds: 0
              timeoutSeconds: 1
              ## This means it has a maximum of 5*30=150 seconds to start up before it fails
              periodSeconds: 5
              failureThreshold: 30


persistence:
  dockerscripts:
    enabled: true
    type: configMap
    identifier: dockerscripts

    defaultmode: 0755

    advancedMounts:
      openwebuimcp: # the controller with the "main" identifier
        docker: # the container with the "main" identifier
          - path: /usr/local/bin/docker-entrypoint.sh
            readOnly: true
            subPath: docker-entrypoint.sh

  config:
    enabled: true
    type: configMap
    name: '{{ include "bjw-s.common.lib.chart.names.fullname" $ }}'

    advancedMounts:
      openwebuimcp: # the controller with the "main" identifier
        mcpo: # the container with the "main" identifier
          - path: /config/config.json
            readOnly: true
            subPath: config.json

  dockercerts:
    enabled: true
    type: emptyDir

    advancedMounts:
      openwebuimcp: # the controller with the "main" identifier
        docker: # the container with the "main" identifier
          - path: /certs/client


        mcpo: # the container with the "main" identifier
          - path: /certs/client

  dockersocket:
    enabled: true
    type: emptyDir

    advancedMounts:
      openwebuimcp: # the controller with the "main" identifier
        docker: # the container with the "main" identifier
          - path: /var/run


        mcpo: # the container with the "main" identifier
          - path: /var/run

  # tls:
  #   enabled: true
  #   type: secret
  #   name: myloginspace-default-certificates
  #   mountPath: /etc/tls

configMaps:
  dockerscripts:
    enabled: true

    data:
      docker-entrypoint.sh: |
        #!/bin/sh
        set -eu

        _tls_ensure_private() {
          local f="$1"; shift
          [ -s "$f" ] || openssl genrsa -out "$f" 4096
        }
        _tls_san() {
          {
            ip -oneline address | awk '{ gsub(/\/.+$/, "", $4); print "IP:" $4 }'
            {
              cat /etc/hostname
              echo 'docker'
              echo 'localhost'
              hostname -f
              hostname -s
            } | sed 's/^/DNS:/'
            [ -z "${DOCKER_TLS_SAN:-}" ] || echo "$DOCKER_TLS_SAN"
          } | sort -u | xargs printf '%s,' | sed "s/,\$//"
        }
        _tls_generate_certs() {
          local dir="$1"; shift

          # if server/{ca,key,cert}.pem && !ca/key.pem, do NOTHING except verify (user likely managing CA themselves)
          # if ca/key.pem || !ca/cert.pem, generate CA public if necessary
          # if ca/key.pem, generate server public
          # if ca/key.pem, generate client public
          # (regenerating public certs every startup to account for SAN/IP changes and/or expiration)

          if [ -s "$dir/server/ca.pem" ] && [ -s "$dir/server/cert.pem" ] && [ -s "$dir/server/key.pem" ] && [ ! -s "$dir/ca/key.pem" ]; then
            openssl verify -CAfile "$dir/server/ca.pem" "$dir/server/cert.pem"
            return 0
          fi

          # https://github.com/FiloSottile/mkcert/issues/174
          local certValidDays='825'

          if [ -s "$dir/ca/key.pem" ] || [ ! -s "$dir/ca/cert.pem" ]; then
            # if we either have a CA private key or do *not* have a CA public key, then we should create/manage the CA
            mkdir -p "$dir/ca"
            _tls_ensure_private "$dir/ca/key.pem"
            openssl req -new -key "$dir/ca/key.pem" \
              -out "$dir/ca/cert.pem" \
              -subj '/CN=docker:dind CA' \
              -x509 \
              -days "$certValidDays" \
              -addext keyUsage=critical,digitalSignature,keyCertSign
          fi

          if [ -s "$dir/ca/key.pem" ]; then
            # if we have a CA private key, we should create/manage a server key
            mkdir -p "$dir/server"
            _tls_ensure_private "$dir/server/key.pem"
            openssl req -new -key "$dir/server/key.pem" \
              -out "$dir/server/csr.pem" \
              -subj '/CN=docker:dind server'
            cat > "$dir/server/openssl.cnf" <<-EOF
              [ x509_exts ]
              extendedKeyUsage = serverAuth
              subjectAltName = $(_tls_san)
            EOF
            openssl x509 -req \
                -in "$dir/server/csr.pem" \
                -CA "$dir/ca/cert.pem" \
                -CAkey "$dir/ca/key.pem" \
                -CAcreateserial \
                -out "$dir/server/cert.pem" \
                -days "$certValidDays" \
                -extfile "$dir/server/openssl.cnf" \
                -extensions x509_exts
            cp "$dir/ca/cert.pem" "$dir/server/ca.pem"
            openssl verify -CAfile "$dir/server/ca.pem" "$dir/server/cert.pem"
          fi

          if [ -s "$dir/ca/key.pem" ]; then
            # if we have a CA private key, we should create/manage a client key
            mkdir -p "$dir/client"
            _tls_ensure_private "$dir/client/key.pem"
            chmod 0644 "$dir/client/key.pem" # openssl defaults to 0600 for the private key, but this one needs to be shared with arbitrary client contexts
            openssl req -new \
                -key "$dir/client/key.pem" \
                -out "$dir/client/csr.pem" \
                -subj '/CN=docker:dind client'
            cat > "$dir/client/openssl.cnf" <<-'EOF'
              [ x509_exts ]
              extendedKeyUsage = clientAuth
            EOF
            openssl x509 -req \
                -in "$dir/client/csr.pem" \
                -CA "$dir/ca/cert.pem" \
                -CAkey "$dir/ca/key.pem" \
                -CAcreateserial \
                -out "$dir/client/cert.pem" \
                -days "$certValidDays" \
                -extfile "$dir/client/openssl.cnf" \
                -extensions x509_exts
            cp "$dir/ca/cert.pem" "$dir/client/ca.pem"
            openssl verify -CAfile "$dir/client/ca.pem" "$dir/client/cert.pem"
          fi
        }

        # no arguments passed
        # or first arg is `-f` or `--some-option`
        if [ "$#" -eq 0 ] || [ "${1#-}" != "$1" ]; then
          # set "dockerSocket" to the default "--host" *unix socket* value (for both standard or rootless)
          uid="$(id -u)"
          if [ "$uid" = '0' ]; then
            dockerSocket='unix:///var/run/docker.sock'
          else
            # if we're not root, we must be trying to run rootless
            : "${XDG_RUNTIME_DIR:=/run/user/$uid}"
            dockerSocket="unix://$XDG_RUNTIME_DIR/docker.sock"
          fi
          case "${DOCKER_HOST:-}" in
            unix://*)
              dockerSocket="$DOCKER_HOST"
              ;;
          esac

          # add our default arguments
          if [ -n "${DOCKER_TLS_CERTDIR:-}" ]; then
            _tls_generate_certs "$DOCKER_TLS_CERTDIR"
            # generate certs and use TLS if requested/possible (default in 19.03+)
            set -- dockerd \
              --host="$dockerSocket" \
              --host=tcp://0.0.0.0:2376 \
              --tlsverify \
              --tlscacert "$DOCKER_TLS_CERTDIR/server/ca.pem" \
              --tlscert "$DOCKER_TLS_CERTDIR/server/cert.pem" \
              --tlskey "$DOCKER_TLS_CERTDIR/server/key.pem" \
              "$@"
            DOCKERD_ROOTLESS_ROOTLESSKIT_FLAGS="${DOCKERD_ROOTLESS_ROOTLESSKIT_FLAGS:-} -p 0.0.0.0:2376:2376/tcp"
          else
            # TLS disabled (-e DOCKER_TLS_CERTDIR='') or missing certs
            set -- dockerd \
              --host="$dockerSocket" \
              --host=tcp://0.0.0.0:2375 \
              "$@"
            DOCKERD_ROOTLESS_ROOTLESSKIT_FLAGS="${DOCKERD_ROOTLESS_ROOTLESSKIT_FLAGS:-} -p 0.0.0.0:2375:2375/tcp"
          fi
        fi

        if [ "$1" = 'dockerd' ]; then
          # explicitly remove Docker's default PID file to ensure that it can start properly if it was stopped uncleanly (and thus didn't clean up the PID file)
          find /run /var/run -iname 'docker*.pid' -delete || :

          # XXX inject "docker-init" (tini) as pid1 to workaround https://github.com/docker-library/docker/issues/318 (zombie container-shim processes)
          set -- docker-init -- "$@"

          iptablesLegacy=
          if [ -n "${DOCKER_IPTABLES_LEGACY+x}" ]; then
            # let users choose explicitly to legacy or not to legacy
            iptablesLegacy="$DOCKER_IPTABLES_LEGACY"
            if [ -n "$iptablesLegacy" ]; then
              modprobe ip_tables || :
              modprobe ip6_tables || :
            else
              modprobe nf_tables || :
            fi
          elif (
            # https://git.netfilter.org/iptables/tree/iptables/nft-shared.c?id=f5cf76626d95d2c491a80288bccc160c53b44e88#n420
            # https://github.com/docker-library/docker/pull/468#discussion_r1442131459
            for f in /proc/net/ip_tables_names /proc/net/ip6_tables_names /proc/net/arp_tables_names; do
              if b="$(cat "$f")" && [ -n "$b" ]; then
                exit 0
              fi
            done
            exit 1
          ); then
            # if we already have any "legacy" iptables rules, we should always use legacy
            iptablesLegacy=1
          elif ! iptables -nL > /dev/null 2>&1; then
            # if iptables fails to run, chances are high the necessary kernel modules aren't loaded (perhaps the host is using xtables, for example)
            # https://github.com/docker-library/docker/issues/350
            # https://github.com/moby/moby/issues/26824
            # https://github.com/docker-library/docker/pull/437#issuecomment-1854900620
            modprobe nf_tables || :
            if ! iptables -nL > /dev/null 2>&1; then
              # might be host has no nf_tables, but Alpine is all-in now (so let's try a legacy fallback)
              modprobe ip_tables || :
              modprobe ip6_tables || :
              if /usr/local/sbin/.iptables-legacy/iptables -nL > /dev/null 2>&1; then
                iptablesLegacy=1
              fi
            fi
          fi
          if [ -n "$iptablesLegacy" ]; then
            # see https://github.com/docker-library/docker/issues/463 (and the dind Dockerfile where this directory is set up)
            export PATH="/usr/local/sbin/.iptables-legacy:$PATH"
          fi
          iptables --version # so users can see whether it's legacy or not

          uid="$(id -u)"
          if [ "$uid" != '0' ]; then
            # if we're not root, we must be trying to run rootless
            if ! command -v rootlesskit > /dev/null; then
              echo >&2 "error: attempting to run rootless dockerd but missing 'rootlesskit' (perhaps the 'docker:dind-rootless' image variant is intended?)"
              exit 1
            fi
            user="$(id -un 2>/dev/null || :)"
            if ! grep -qE "^($uid${user:+|$user}):" /etc/subuid || ! grep -qE "^($uid${user:+|$user}):" /etc/subgid; then
              echo >&2 "error: attempting to run rootless dockerd but missing necessary entries in /etc/subuid and/or /etc/subgid for $uid"
              exit 1
            fi
            : "${XDG_RUNTIME_DIR:=/run/user/$uid}"
            export XDG_RUNTIME_DIR
            if ! mkdir -p "$XDG_RUNTIME_DIR" || [ ! -w "$XDG_RUNTIME_DIR" ] || ! mkdir -p "$HOME/.local/share/docker" || [ ! -w "$HOME/.local/share/docker" ]; then
              echo >&2 "error: attempting to run rootless dockerd but need writable HOME ($HOME) and XDG_RUNTIME_DIR ($XDG_RUNTIME_DIR) for user $uid"
              exit 1
            fi
            if [ -f /proc/sys/kernel/unprivileged_userns_clone ] && unprivClone="$(cat /proc/sys/kernel/unprivileged_userns_clone)" && [ "$unprivClone" != '1' ]; then
              echo >&2 "error: attempting to run rootless dockerd but need 'kernel.unprivileged_userns_clone' (/proc/sys/kernel/unprivileged_userns_clone) set to 1"
              exit 1
            fi
            if [ -f /proc/sys/user/max_user_namespaces ] && maxUserns="$(cat /proc/sys/user/max_user_namespaces)" && [ "$maxUserns" = '0' ]; then
              echo >&2 "error: attempting to run rootless dockerd but need 'user.max_user_namespaces' (/proc/sys/user/max_user_namespaces) set to a sufficiently large value"
              exit 1
            fi
            # TODO overlay support detection?
            exec rootlesskit \
              --net="${DOCKERD_ROOTLESS_ROOTLESSKIT_NET:-vpnkit}" \
              --mtu="${DOCKERD_ROOTLESS_ROOTLESSKIT_MTU:-1500}" \
              --disable-host-loopback \
              --copy-up=/etc \
              --copy-up=/run \
              ${DOCKERD_ROOTLESS_ROOTLESSKIT_FLAGS:-} \
              "$@"
          elif [ -x '/usr/local/bin/dind' ]; then
            # if we have the (mostly defunct now) Docker-in-Docker wrapper script, use it
            set -- '/usr/local/bin/dind' "$@"
          fi
        else
          # if it isn't `dockerd` we're trying to run, pass it through `docker-entrypoint.sh` so it gets `DOCKER_HOST` set appropriately too
          set -- docker-entrypoint.sh "$@"
        fi

        exec "$@"

  config:
    # -- Enables or disables the configMap
    enabled: true
    # -- Labels to add to the configMap
    labels: {}
    # -- Annotations to add to the configMap
    annotations: {}
    # -- configMap data content. Helm template enabled.
    data:
      config.json: |
        {
          "mcpServers": {
            "crypto-feargreed-mcp": {
              "command": "docker",
              "args": [
                "run",
                "-i",
                "--rm",
                "ghcr.io/metorial/mcp-container--kukapay--crypto-feargreed-mcp--crypto-feargreed-mcp",
                "python main.py"
              ],
              "env": {}
            },
            "code-sandbox": {
              "command": "docker",
              "args": [
                "run", "--rm", "--interactive",
                "--read-only",
                "--tmpfs", "/tmp",
                "--tmpfs", "/app/tmp",
                "--memory", "512m",
                "--cpus", "0.5",
                "--cap-drop", "ALL",
                "ghcr.io/timlikesai/code-sandbox-mcp:latest"
              ]
            }
          }
        }


rbac:
  roles:
    kubedock:
      # -- Enables or disables the Role. Can be templated.
      enabled: true
      # -- Set to Role,ClusterRole
      type: Role
      rules:
        - apiGroups:
            - ''
          resources: 
            - pods
          verbs: 
            - create
            - get
            - list
            - delete
            - watch

        - apiGroups:
            - ''
          resources:
            - pods/log
          verbs:
            - list
            - get

        - apiGroups:
            - ''
          resources: 
            - pods/exec
          verbs:
            - create

        - apiGroups:
            - ''
          resources:
            - services
          verbs:
            - create
            - get
            - list
            - delete

        - apiGroups:
            - ''
          resources:
            - configmaps
          verbs:
            - create
            - get
            - list
            - delete

  bindings:
    kubedock:
      # -- Enables or disables the Role. Can be templated.
      enabled: true
      # -- Set to RoleBinding,ClusterRoleBinding
      type: RoleBinding
      # -- Can be an identifier of rbac.roles or a custom name and kind
      roleRef:
        identifier: kubedock
      # -- If using an identifier it will be automatically filled, otherwise every key will need to be explicitly declared
      subjects:
        - identifier: kubedock

serviceAccount:
  kubedock:
    forceRename: kubedock
    enabled: true
    annotations: {}
    labels: {}


oauth:
  indexKey: openwebui
  scopes: ''
  groups: []
  authentik:
    urlBase: https://idp.mylogin.space



cluster:
  name: k8s
  domain: cluster.local

datacenter: dc1

region: yxl


service:
  openwebuimcp:
    # -- Enables or disables the service
    enabled: true

    # -- Configure which controller this service should target
    controller: openwebuimcp

    # -- Make this the primary service for this controller (used in probes, notes, etc...).
    # If there is more than 1 service targeting the controller, make sure that only 1 service is
    # marked as primary.
    primary: true

    # -- Set the service type
    type: ClusterIP

    # -- Specify the externalTrafficPolicy for the service. Options: Cluster, Local
    # -- [[ref](https://kubernetes.io/docs/tutorials/services/source-ip/)]
    #externalTrafficPolicy: Local

    # -- Specify the ip policy. Options: SingleStack, PreferDualStack, RequireDualStack
    ipFamilyPolicy:
    # -- The ip families that should be used. Options: IPv4, IPv6
    ipFamilies: []

    # -- Provide additional annotations which may be required.
    #annotations:

    # -- Provide additional labels which may be required.
    labels: {}

    # -- Allow adding additional match labels
    extraSelectorLabels: {}

    ports:
      http:
        # -- Enables or disables the port
        enabled: true

        # -- Make this the primary port (used in probes, notes, etc...)
        # If there is more than 1 service, make sure that only 1 port is marked as primary.
        primary: false

        # -- The port number
        port: 80

        targetPort: 8000

        # -- Port protocol.
        # Support values are `HTTP`, `HTTPS`, `TCP` and `UDP`.
        # HTTP and HTTPS spawn a TCP service and get used for internal URL and name generation
        protocol: TCP

        # -- Specify the appProtocol value for the Service.
        # [[ref]](https://kubernetes.io/docs/concepts/services-networking/service/#application-protocol)
        appProtocol:



open-webui:
  nameOverride: ""
  namespaceOverride: ""

  enabled: false
  ollama:
    # -- Automatically install Ollama Helm chart from https://otwld.github.io/ollama-helm/. Use [Helm Values](https://github.com/otwld/ollama-helm/#helm-values) to configure
    enabled: false


  pipelines:
    # -- Automatically install Pipelines chart to extend Open WebUI functionality using Pipelines: https://github.com/open-webui/pipelines
    enabled: true
    # -- This section can be used to pass required environment variables to your pipelines (e.g. Langfuse hostname)
    extraEnvVars: []

  tika:
    # -- Automatically install Apache Tika to extend Open WebUI
    enabled: false

  # -- A list of Ollama API endpoints. These can be added in lieu of automatically installing the Ollama Helm chart, or in addition to it.
  ollamaUrls: []

  # -- Disables taking Ollama Urls from `ollamaUrls`  list
  ollamaUrlsFromExtraEnv: false

  websocket:
    # -- Enables websocket support in Open WebUI with env `ENABLE_WEBSOCKET_SUPPORT`
    enabled: false
    # -- Specifies the websocket manager to use with env `WEBSOCKET_MANAGER`: redis (default)
    manager: redis
    # -- Specifies the URL of the Redis instance for websocket communication. Template with `redis://[:<password>@]<hostname>:<port>/<db>`
    url: redis://open-webui-redis:6379/0
    # -- Node selector for websocket pods
    nodeSelector: {}
    # -- Deploys a redis
    redis:
      # -- Enable redis installation
      enabled: false
      # -- Redis name
      name: open-webui-redis
      # -- Redis labels
      labels: {}
      # -- Redis annotations
      annotations: {}
      # -- Redis pod
      pods:
        # -- Redis pod labels
        labels: {}
        # -- Redis pod annotations
        annotations: {}
      # -- Redis image
      image:
        repository: redis
        tag: 7.4.2-alpine3.21
        pullPolicy: IfNotPresent
      # -- Redis command (overrides default)
      command: []
      # -- Redis arguments (overrides default)
      args: []
      # -- Redis resources
      resources: {}
      # -- Redis service
      service:
        # -- Redis container/target port
        containerPort: 6379
        # -- Redis service type
        type: ClusterIP
        # -- Redis service labels
        labels: {}
        # -- Redis service annotations
        annotations: {}
        # -- Redis service port
        port: 6379
        # -- Redis service node port. Valid only when type is `NodePort`
        nodePort: ""
      # -- Redis tolerations for pod assignment
      tolerations: []

      # -- Redis affinity for pod assignment
      affinity: {}

      # -- Redis security context
      securityContext:
        {}
        # runAsUser: 999
        # runAsGroup: 1000

  # -- Deploys a Redis cluster with subchart 'redis' from bitnami
  redis-cluster:
    # -- Enable Redis installation
    enabled: false
    # -- Redis cluster name (recommended to be 'open-webui-redis')
    # - In this case, redis url will be 'redis://open-webui-redis-master:6379/0' or 'redis://[:<password>@]open-webui-redis-master:6379/0'
    fullnameOverride: open-webui-redis
    
    # -- Specifies the URL of the Redis instance for websocket communication. Template with `redis://[:<password>@]<hostname>:<port>/<db>`
    url: redis://open-webui-redis:6379/0

  # -- Value of cluster domain
  clusterDomain: cluster.local

  annotations: {}
  podAnnotations: {}
  podLabels: {}
  replicaCount: 1
  # -- Revision history limit for the workload manager (deployment).
  revisionHistoryLimit: 10
  # -- Priority class name for the Open WebUI pods
  priorityClassName: ""
  # -- Strategy for updating the workload manager: deployment or statefulset
  strategy: {}
  # -- Open WebUI image tags can be found here: https://github.com/open-webui/open-webui
  image:
    repository: ghcr.io/open-webui/open-webui
    tag: ""
    pullPolicy: "IfNotPresent"

  # -- Open WebUI container command (overrides default entrypoint)
  command: []
  # -- Open WebUI container arguments (overrides default)
  args: []

  serviceAccount:
    enable: true
    name: ""
    annotations: {}
    automountServiceAccountToken: false

  # -- Configure imagePullSecrets to use private registry
  # ref: <https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry>
  imagePullSecrets: []
  # imagePullSecrets:
  # - name: myRegistryKeySecretName

  # -- Probe for liveness of the Open WebUI container
  # ref: <https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes>
  livenessProbe: {}
  # livenessProbe:
  #   httpGet:
  #     path: /health
  #     port: http
  #   failureThreshold: 1
  #   periodSeconds: 10

  # -- Probe for readiness of the Open WebUI container
  # ref: <https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes>
  readinessProbe: {}
  # readinessProbe:
  #   httpGet:
  #     path: /health/db
  #     port: http
  #   failureThreshold: 1
  #   periodSeconds: 10

  # -- Probe for startup of the Open WebUI container
  # ref: <https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes>
  startupProbe: {}
  # startupProbe:
  #   httpGet:
  #     path: /health
  #     port: http
  #   initialDelaySeconds: 30
  #   periodSeconds: 5
  #   failureThreshold: 20

  resources: {}

  copyAppData:
    # -- Open WebUI copy-app-data init container command (overrides default)
    command: []
    # -- Open WebUI copy-app-data init container arguments (overrides default)
    args: []

    resources: {}

  managedCertificate:
    enabled: false
    name: "mydomain-chat-cert" # You can override this name if needed
    domains:
      - chat.example.com # update to your real domain

  ingress:
    enabled: false
    class: ""
    # -- Use appropriate annotations for your Ingress controller, e.g., for NGINX:
    annotations: {}
    #   # Example for GKE Ingress
    #   kubernetes.io/ingress.class: "gce"
    #   kubernetes.io/ingress.global-static-ip-name: "open-webui-external-ip"   #  you need to create this address in GCP console
    #   # Force HTTP to redirect to HTTPS
    #   nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    #   nginx.ingress.kubernetes.io/ssl-redirect: "true"
    #   nginx.ingress.kubernetes.io/permanent-redirect: "https://chat.example.com"
    #   networking.gke.io/managed-certificates: "mydomain-chat-cert"
    #   # nginx.ingress.kubernetes.io/rewrite-target: /
    host: "chat.example.com" # update to your real domain
    additionalHosts: []
    tls: false
    existingSecret: ""

    # -- Additional custom labels to add to the Ingress metadata
    # Useful for tagging, selecting, or applying policies to the Ingress via labels.
    extraLabels: {}
    # extraLabels:
    #   app.kubernetes.io/environment: "staging"

  persistence:
    enabled: true
    size: 2Gi
    # -- Use existingClaim if you want to re-use an existing Open WebUI PVC instead of creating a new one
    existingClaim: ""
    # -- Subdirectory of Open WebUI PVC to mount. Useful if root directory is not empty.
    subPath: ""
    # -- If using multiple replicas, you must update accessModes to ReadWriteMany
    accessModes:
      - ReadWriteOnce
    storageClass: ""
    selector: {}
    annotations: {}
    # -- Sets the storage provider, availables values are `local`, `s3`, `gcs` or `azure`
    provider: local
    s3:
      # -- Sets the access key ID for S3 storage
      # @section -- Amazon S3 Storage configuration
      accessKey: ""
      # -- Sets the secret access key for S3 storage (ignored if secretKeyExistingSecret is set)
      # @section -- Amazon S3 Storage configuration
      secretKey: ""
      # -- Set the secret access key for S3 storage from existing k8s secret
      # @section -- Amazon S3 Storage configuration
      accessKeyExistingSecret: ""
      # -- Set the secret access key for S3 storage from existing k8s secret key
      # @section -- Amazon S3 Storage configuration
      accessKeyExistingAccessKey: ""
      # -- Set the secret key for S3 storage from existing k8s secret
      # @section -- Amazon S3 Storage configuration
      secretKeyExistingSecret: ""
      # -- Set the secret key for S3 storage from existing k8s secret key
      # @section -- Amazon S3 Storage configuration
      secretKeyExistingSecretKey: ""
      # -- Sets the endpoint url for S3 storage
      # @section -- Amazon S3 Storage configuration
      endpointUrl: ""
      # -- Sets the region name for S3 storage
      # @section -- Amazon S3 Storage configuration
      region: ""
      # -- Sets the bucket name for S3 storage
      # @section -- Amazon S3 Storage configuration
      bucket: ""
      # -- Sets the key prefix for a S3 object
      # @section -- Amazon S3 Storage configuration
      keyPrefix: ""
    gcs:
      # -- Contents of Google Application Credentials JSON file (ignored if appCredentialsJsonExistingSecret is set). Optional - if not provided, credentials will be taken from the environment. User credentials if run locally and Google Metadata server if run on a Google Compute Engine. File can be generated for a service account following this guide: https://developers.google.com/workspace/guides/create-credentials#service-account
      # @section -- Google Cloud Storage configuration
      appCredentialsJson: ""
      # -- Set the Google Application Credentials JSON file for Google Cloud Storage from existing secret
      # @section -- Google Cloud Storage configuration
      appCredentialsJsonExistingSecret: ""
      # -- Set the Google Application Credentials JSON file for Google Cloud Storage from existing secret key
      # @section -- Google Cloud Storage configuration
      appCredentialsJsonExistingSecretKey: ""
      # -- Sets the bucket name for Google Cloud Storage. Bucket must already exist
      # @section -- Google Cloud Storage configuration
      bucket: ""
    azure:
      # -- Sets the endpoint URL for Azure Storage
      # @section -- Azure Storage configuration
      endpointUrl: ""
      # -- Sets the container name for Azure Storage
      # @section -- Azure Storage configuration
      container: ""
      # -- Set the access key for Azure Storage (ignored if keyExistingSecret is set). Optional - if not provided, credentials will be taken from the environment. User credentials if run locally and Managed Identity if run in Azure services
      # @section -- Azure Storage configuration
      key: ""
      # -- Set the access key for Azure Storage from existing secret
      # @section -- Azure Storage configuration
      keyExistingSecret: ""
      # -- Set the access key for Azure Storage from existing secret key
      # @section -- Azure Storage configuration
      keyExistingSecretKey: ""

  # -- Node labels for pod assignment.
  nodeSelector: {}

  # -- Tolerations for pod assignment
  tolerations: []

  # -- Affinity for pod assignment
  affinity: {}

  # -- Topology Spread Constraints for pod assignment
  topologySpreadConstraints: []

  # -- HostAliases to be added to hosts-file of each container
  hostAliases: []

  # -- Service values to expose Open WebUI pods to cluster
  service:
    type: ClusterIP
    annotations: {}
    port: 80
    containerPort: 8080
    nodePort: ""
    labels: {}
    loadBalancerClass: ""

  # -- Enables the use of OpenAI APIs
  enableOpenaiApi: true

  # -- OpenAI base API URL to use. Defaults to the Pipelines service endpoint when Pipelines are enabled, and "https://api.openai.com/v1" if Pipelines are not enabled and this value is blank
  openaiBaseApiUrl: "https://api.openai.com/v1"

  # -- OpenAI base API URLs to use. Overwrites the value in openaiBaseApiUrl if set
  openaiBaseApiUrls:
    []
    # - "https://api.openai.com/v1"
    # - "https://api.company.openai.com/v1"

  # -- Env vars added to the Open WebUI deployment. Most up-to-date environment variables can be found here: https://docs.openwebui.com/getting-started/env-configuration/
  extraEnvVars:
    # -- Default API key value for Pipelines. Should be updated in a production deployment, or be changed to the required API key if not using Pipelines
    - name: OPENAI_API_KEY
      value: "0p3n-w3bu!"
    # valueFrom:
    #   secretKeyRef:
    #     name: pipelines-api-key
    #     key: api-key
    # - name: OPENAI_API_KEY
    #   valueFrom:
    #     secretKeyRef:
    #       name: openai-api-key
    #       key: api-key
    # - name: OLLAMA_DEBUG
    #   value: "1"

  # -- Env vars added to the Open WebUI deployment, common across environments. Most up-to-date environment variables can be found here: https://docs.openwebui.com/getting-started/env-configuration/ (caution: environment variables defined in both `extraEnvVars` and `commonEnvVars` will result in a conflict. Avoid duplicates)
  commonEnvVars: []
    # - name: RAG_EMBEDDING_ENGINE
    #   value: "openai"

  # -- Env vars added from configmap or secret to the Open WebUI deployment. Most up-to-date environment variables can be found here: https://docs.openwebui.com/getting-started/env-configuration/ (caution: `extraEnvVars` will take precedence over the value from `extraEnvFrom`)
  extraEnvFrom: []
    # - configMapRef:
    #     name: my-config
    # - secretRef:
    #     name: my-secret

  # -- Configure runtime class
  # ref: <https://kubernetes.io/docs/concepts/containers/runtime-class/>
  runtimeClassName: ""

  # -- Configure container volume mounts
  # ref: <https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/>
  volumeMounts:
    initContainer: []
    # - name: ""
    #   mountPath: ""
    container: []
    # - name: ""
    #   mountPath: ""

  # -- Additional init containers to add to the deployment/statefulset
  # ref: <https://kubernetes.io/docs/concepts/workloads/pods/init-containers/>
  extraInitContainers: []
  # - name: custom-init
  #   image: busybox:latest
  #   command: ['sh', '-c', 'echo "Custom init container running"']
  #   volumeMounts:
  #   - name: data
  #     mountPath: /data

  # -- Configure pod volumes
  # ref: <https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/>
  volumes: []
  # - name: ""
  #   configMap:
  #     name: ""
  # - name: ""
  #   emptyDir: {}

  # -- Configure pod security context
  # ref: <https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container>
  podSecurityContext:
    {}
    # fsGroupChangePolicy: Always
    # sysctls: []
    # supplementalGroups: []
    # fsGroup: 1001

  # -- Configure container security context
  # ref: <https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-containe>
  containerSecurityContext:
    {}
    # runAsUser: 1001
    # runAsGroup: 1001
    # runAsNonRoot: true
    # privileged: false
    # allowPrivilegeEscalation: false
    # readOnlyRootFilesystem: false
    # capabilities:
    #   drop:
    #     - ALL
    # seccompProfile:
    #   type: "RuntimeDefault"

  sso:
    # -- **Enable SSO authentication globally** must enable to use SSO authentication
    # @section -- SSO Configuration
    enabled: false
    # -- Enable account creation when logging in with OAuth (distinct from regular signup)
    # @section -- SSO Configuration
    enableSignup: false
    # -- Allow logging into accounts that match email from OAuth provider (considered insecure)
    # @section -- SSO Configuration
    mergeAccountsByEmail: false
    # -- Enable OAuth role management through access token roles claim
    # @section -- SSO Configuration
    enableRoleManagement: false
    # -- Enable OAuth group management through access token groups claim
    # @section -- SSO Configuration
    enableGroupManagement: false

    google:
      # -- Enable Google OAuth
      # @section -- Google OAuth configuration
      enabled: false

    microsoft:
      # -- Enable Microsoft OAuth
      # @section -- Microsoft OAuth configuration
      enabled: false

    github:
      # -- Enable GitHub OAuth
      # @section -- GitHub OAuth configuration
      enabled: false
      # -- GitHub OAuth client ID
      # @section -- GitHub OAuth configuration
      clientId: ""
      # -- GitHub OAuth client secret (ignored if clientExistingSecret is set)
      # @section -- GitHub OAuth configuration
      clientSecret: ""
      # -- GitHub OAuth client secret from existing secret
      # @section -- GitHub OAuth configuration
      clientExistingSecret: ""
      # -- GitHub OAuth client secret key from existing secret
      # @section -- GitHub OAuth configuration
      clientExistingSecretKey: ""

    oidc:
      # -- Enable OIDC authentication
      # @section -- OIDC configuration
      enabled: false
      # -- OIDC client ID
      # @section -- OIDC configuration
      clientId: ""
      # -- OIDC client secret (ignored if clientExistingSecret is set)
      # @section -- OIDC configuration
      clientSecret: ""
      # -- OICD client secret from existing secret
      # @section -- OIDC configuration
      clientExistingSecret: ""
      # -- OIDC client secret key from existing secret
      # @section -- OIDC configuration
      clientExistingSecretKey: ""
      # -- OIDC provider well known URL
      # @section -- OIDC configuration
      providerUrl: ""
      # -- Name of the provider to show on the UI
      # @section -- OIDC configuration
      providerName: "SSO"
      # -- Scopes to request (space-separated).
      # @section -- OIDC configuration
      scopes: "openid email profile"

    roleManagement:
      # -- The claim that contains the roles (can be nested, e.g., user.roles)
      # @section -- Role management configuration
      rolesClaim: "roles"
      # -- Comma-separated list of roles allowed to log in (receive open webui role user)
      # @section -- Role management configuration
      allowedRoles: ""
      # -- Comma-separated list of roles allowed to log in as admin (receive open webui role admin)
      # @section -- Role management configuration
      adminRoles: ""

    groupManagement:
      # -- The claim that contains the groups (can be nested, e.g., user.memberOf)
      # @section -- SSO Configuration
      groupsClaim: "groups"

    trustedHeader:
      # -- Enable trusted header authentication
      # @section -- SSO trusted header authentication
      enabled: false
      # -- Header containing the user's email address
      # @section -- SSO trusted header authentication
      emailHeader: ""
      # -- Header containing the user's name (optional, used for new user creation)
      # @section -- SSO trusted header authentication
      nameHeader: ""

  # -- Extra resources to deploy with Open WebUI
  extraResources:
    []
    # - apiVersion: v1
    #   kind: ConfigMap
    #   metadata:
    #     name: example-configmap
    #   data:
    #     example-key: example-value

  # -- Configure database URL, needed to work with Postgres (example: `postgresql://<user>:<password>@<service>:<port>/<database>`), leave empty to use the default sqlite database
  databaseUrl: ""

  # -- Postgresql configuration (see. https://artifacthub.io/packages/helm/bitnami/postgresql)
  postgresql:
    enabled: false
    architecture: standalone
    auth:
      database: open-webui
      postgresPassword: 0p3n-w3bu!
      username: open-webui
      password: 0p3n-w3bu!
    primary:
      persistence:
        size: 1Gi
      resources:
        requests:
          memory: 256Mi
          cpu: 250m
        limits:
          memory: 512Mi
          cpu: 500m

  # Configure Application logging levels (see. https://docs.openwebui.com/getting-started/advanced-topics/logging#-logging-levels-explained)
  logging:
    # -- Set the global log level ["notset", "debug", "info" (default), "warning", "error", "critical"]
    # @section -- Logging configuration
    level: ""

    # Optional granularity: override log levels per subsystem/component
    # if not set, it will use the global level (see. https://docs.openwebui.com/getting-started/advanced-topics/logging#%EF%B8%8F-appbackend-specific-logging-levels)
    components:
      # -- Set the log level for the Audio processing component
      # @section -- Logging configuration
      audio: ""
      # -- Set the log level for the ComfyUI Integration component
      # @section -- Logging configuration
      comfyui: ""
      # -- Set the log level for the Configuration Management component
      # @section -- Logging configuration
      config: ""
      # -- Set the log level for the Database Operations (Peewee) component
      # @section -- Logging configuration
      db: ""
      # -- Set the log level for the Image Generation component
      # @section -- Logging configuration
      images: ""
      # -- Set the log level for the Main Application Execution component
      # @section -- Logging configuration
      main: ""
      # -- Set the log level for the Model Management component
      # @section -- Logging configuration
      models: ""
      # -- Set the log level for the Ollama Backend Integration component
      # @section -- Logging configuration
      ollama: ""
      # -- Set the log level for the OpenAI API Integration component
      # @section -- Logging configuration
      openai: ""
      # -- Set the log level for the Retrieval-Augmented Generation (RAG) component
      # @section -- Logging configuration
      rag: ""
      # -- Set the log level for the Authentication Webhook component
      # @section -- Logging configuration
      webhook: ""
